{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "source": [
    "### 1. 상태 표현\n",
    "기존에는 그리드 맵 상의 (x, y) 좌표만을 사용했지만, 제어할 수 없는 속도 값도 상태에 포함될 필요가 있습니다. 그렇게 되면 상태는 (x, y, v)와 같은 형태를 가질 것입니다.\n",
    "\n",
    "### 2. 행동 공간\n",
    "행동은 '왼쪽으로 틀기'와 '오른쪽으로 틀기' 두 가지만 가능합니다. 이를 감안하면, 각 상태에서 가능한 행동은 이 두 가지로 제한됩니다.\n",
    "\n",
    "### 3. 보상 구조\n",
    "에이전트가 목적지에 빨리 도착하면 큰 보상을 받고, 잘못된 방향으로 가면 패널티를 받는 구조를 만들면 됩니다.\n",
    "\n",
    "이렇게 상태와 행동, 그리고 보상을 정의한 후에는 이를 기반으로 RL 환경을 구축할 수 있습니다. 이 환경에서 에이전트가 학습을 통해 목적지까지 빠르게 도달하는 방법을 배울 수 있을 것입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-13T07:54:58.173702Z",
     "iopub.status.busy": "2023-10-13T07:54:58.173335Z",
     "iopub.status.idle": "2023-10-13T07:54:58.179219Z",
     "shell.execute_reply": "2023-10-13T07:54:58.178080Z",
     "shell.execute_reply.started": "2023-10-13T07:54:58.173676Z"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'apex'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_4324\\2499919232.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mrandom\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mcollections\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdeque\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mapex\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mamp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mgym\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'apex'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import random\n",
    "from collections import deque\n",
    "from apex import amp\n",
    "\n",
    "import gym\n",
    "from gym import spaces\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "from matplotlib.animation import FuncAnimation\n",
    "from matplotlib.backends.backend_agg import FigureCanvasAgg as FigureCanvas\n",
    "\n",
    "from scipy.ndimage import label\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from tqdm import tqdm \n",
    "from IPython.display import clear_output\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting apex\n",
      "  Using cached apex-0.9.10.dev0-py3-none-any.whl\n",
      "Collecting pyramid>1.1.2\n",
      "  Using cached pyramid-2.0.2-py3-none-any.whl (247 kB)\n",
      "Requirement already satisfied: requests in c:\\users\\82108\\anaconda3\\envs\\230604\\lib\\site-packages (from apex) (2.28.1)\n",
      "Collecting zope.sqlalchemy\n",
      "  Using cached zope.sqlalchemy-3.1-py3-none-any.whl (23 kB)\n",
      "Collecting wtforms-recaptcha\n",
      "  Using cached wtforms_recaptcha-0.3.2-py2.py3-none-any.whl (7.5 kB)\n",
      "Collecting wtforms\n",
      "  Using cached WTForms-3.0.1-py3-none-any.whl (136 kB)\n",
      "Collecting velruse>=1.0.3\n",
      "  Using cached velruse-1.1.1-py3-none-any.whl\n",
      "Collecting cryptacular\n",
      "  Using cached cryptacular-1.6.2.tar.gz (75 kB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Collecting pyramid-mailer\n",
      "  Using cached pyramid_mailer-0.15.1-py2.py3-none-any.whl (19 kB)\n",
      "Collecting plaster\n",
      "  Using cached plaster-1.1.2-py2.py3-none-any.whl (11 kB)\n",
      "Collecting venusian>=1.0\n",
      "  Using cached venusian-3.1.0-py3-none-any.whl (13 kB)\n",
      "Requirement already satisfied: setuptools in c:\\users\\82108\\anaconda3\\envs\\230604\\lib\\site-packages (from pyramid>1.1.2->apex) (65.6.3)\n",
      "Requirement already satisfied: zope.interface>=3.8.0 in c:\\users\\82108\\anaconda3\\envs\\230604\\lib\\site-packages (from pyramid>1.1.2->apex) (6.1)\n",
      "Collecting hupper>=1.5\n",
      "  Using cached hupper-1.12-py3-none-any.whl (22 kB)\n",
      "Collecting translationstring>=0.4\n",
      "  Using cached translationstring-1.4-py2.py3-none-any.whl (15 kB)\n",
      "Collecting webob>=1.8.3\n",
      "  Using cached WebOb-1.8.7-py2.py3-none-any.whl (114 kB)\n",
      "Collecting zope.deprecation>=3.5.0\n",
      "  Using cached zope.deprecation-5.0-py3-none-any.whl (10 kB)\n",
      "Collecting plaster-pastedeploy\n",
      "  Using cached plaster_pastedeploy-1.0.1-py2.py3-none-any.whl (7.8 kB)\n",
      "Collecting python3-openid\n",
      "  Using cached python3_openid-3.2.0-py3-none-any.whl (133 kB)\n",
      "Requirement already satisfied: requests-oauthlib in c:\\users\\82108\\anaconda3\\envs\\230604\\lib\\site-packages (from velruse>=1.0.3->apex) (1.3.1)\n",
      "Collecting anykeystore\n",
      "  Using cached anykeystore-0.2-py3-none-any.whl\n",
      "Collecting pbkdf2\n",
      "  Using cached pbkdf2-1.3-py3-none-any.whl\n",
      "Collecting transaction\n",
      "  Downloading transaction-4.0-py3-none-any.whl (46 kB)\n",
      "     ---------------------------------------- 46.6/46.6 kB 2.3 MB/s eta 0:00:00\n",
      "Collecting repoze.sendmail>=4.1\n",
      "  Using cached repoze.sendmail-4.4.1-py2.py3-none-any.whl (41 kB)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\82108\\anaconda3\\envs\\230604\\lib\\site-packages (from requests->apex) (2022.12.7)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\82108\\anaconda3\\envs\\230604\\lib\\site-packages (from requests->apex) (1.26.14)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\82108\\anaconda3\\envs\\230604\\lib\\site-packages (from requests->apex) (3.4)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\82108\\anaconda3\\envs\\230604\\lib\\site-packages (from requests->apex) (2.0.4)\n",
      "Requirement already satisfied: MarkupSafe in c:\\users\\82108\\anaconda3\\envs\\230604\\lib\\site-packages (from wtforms->apex) (2.1.1)\n",
      "Requirement already satisfied: packaging in c:\\users\\82108\\anaconda3\\envs\\230604\\lib\\site-packages (from zope.sqlalchemy->apex) (22.0)\n",
      "Requirement already satisfied: SQLAlchemy!=1.4.0,!=1.4.1,!=1.4.2,!=1.4.3,!=1.4.4,!=1.4.5,!=1.4.6,>=1.1 in c:\\users\\82108\\anaconda3\\envs\\230604\\lib\\site-packages (from zope.sqlalchemy->apex) (1.4.49)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\82108\\anaconda3\\envs\\230604\\lib\\site-packages (from SQLAlchemy!=1.4.0,!=1.4.1,!=1.4.2,!=1.4.3,!=1.4.4,!=1.4.5,!=1.4.6,>=1.1->zope.sqlalchemy->apex) (2.0.2)\n",
      "Requirement already satisfied: importlib-metadata in c:\\users\\82108\\anaconda3\\envs\\230604\\lib\\site-packages (from SQLAlchemy!=1.4.0,!=1.4.1,!=1.4.2,!=1.4.3,!=1.4.4,!=1.4.5,!=1.4.6,>=1.1->zope.sqlalchemy->apex) (4.11.3)\n",
      "Collecting PasteDeploy>=2.0\n",
      "  Using cached PasteDeploy-3.0.1-py3-none-any.whl (16 kB)\n",
      "Requirement already satisfied: defusedxml in c:\\users\\82108\\anaconda3\\envs\\230604\\lib\\site-packages (from python3-openid->velruse>=1.0.3->apex) (0.7.1)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\82108\\anaconda3\\envs\\230604\\lib\\site-packages (from requests-oauthlib->velruse>=1.0.3->apex) (3.2.2)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\82108\\anaconda3\\envs\\230604\\lib\\site-packages (from importlib-metadata->SQLAlchemy!=1.4.0,!=1.4.1,!=1.4.2,!=1.4.3,!=1.4.4,!=1.4.5,!=1.4.6,>=1.1->zope.sqlalchemy->apex) (3.11.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in c:\\users\\82108\\anaconda3\\envs\\230604\\lib\\site-packages (from importlib-metadata->SQLAlchemy!=1.4.0,!=1.4.1,!=1.4.2,!=1.4.3,!=1.4.4,!=1.4.5,!=1.4.6,>=1.1->zope.sqlalchemy->apex) (4.3.0)\n",
      "Building wheels for collected packages: cryptacular\n",
      "  Building wheel for cryptacular (pyproject.toml): started\n",
      "  Building wheel for cryptacular (pyproject.toml): finished with status 'done'\n",
      "Failed to build cryptacular\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: Building wheel for cryptacular failed: [Errno 2] No such file or directory: 'C:\\\\Users\\\\82108\\\\AppData\\\\Local\\\\Temp\\\\pip-wheel-eggfmsg_\\\\cryptacular-1.6.2-cp37-cp37m-win_amd64.whl'\n",
      "ERROR: Could not build wheels for cryptacular, which is required to install pyproject.toml-based projects\n"
     ]
    }
   ],
   "source": [
    "!pip install apex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-13T07:43:01.133440Z",
     "iopub.status.busy": "2023-10-13T07:43:01.132749Z",
     "iopub.status.idle": "2023-10-13T07:43:01.147769Z",
     "shell.execute_reply": "2023-10-13T07:43:01.146636Z",
     "shell.execute_reply.started": "2023-10-13T07:43:01.133398Z"
    }
   },
   "outputs": [],
   "source": [
    "class SimpleOceanEnv(gym.Env):\n",
    "    def __init__(self, size, stop_n):\n",
    "        super(SimpleOceanEnv, self).__init__()\n",
    "        \n",
    "        # 행동공간: 위, 아래, 왼쪽, 오른쪽\n",
    "        self.action_space = spaces.Discrete(4)\n",
    "        \n",
    "        # 상태공간: 그리드 위치\n",
    "        self.observation_space = spaces.Tuple((\n",
    "            spaces.Discrete(size),\n",
    "            spaces.Discrete(size)\n",
    "        ))\n",
    "\n",
    "        self.size = size\n",
    "        self.stop_n = stop_n\n",
    "        self.agent_position = None\n",
    "        self.start_position = None\n",
    "        self.end_position = None  # 목표 위치 초기화\n",
    "        self.total_reward = 0  # 에이전트의 총 보상을 초기화\n",
    "        self.previous_action = None\n",
    "        self.recently_visited = []\n",
    "        \n",
    "        # 임의로 바다(1)와 육지(0)를 생성\n",
    "        self.map = np.random.choice([0, 1], size*size, p=[0.1, 0.9]).reshape(size, size)\n",
    "        self.generate_map()\n",
    "        \n",
    "    def generate_map(self):\n",
    "        self.map = np.ones((self.size, self.size))  # 전체를 바다로 초기화\n",
    "\n",
    "        num_landmasses = 10  # 생성할 육지 덩어리의 개수\n",
    "\n",
    "        for _ in range(num_landmasses):\n",
    "            # 육지 덩어리의 시작점을 무작위로 선택\n",
    "            start_x = np.random.randint(0, self.size)\n",
    "            start_y = np.random.randint(0, self.size)\n",
    "\n",
    "            # 육지 덩어리의 크기를 무작위로 선택 (예: 10 ~ 20)\n",
    "            landmass_size = np.random.randint(10, 20)\n",
    "\n",
    "            for _ in range(landmass_size):\n",
    "                direction = np.random.randint(0, 4)  # 랜덤한 방향 선택: 0=위, 1=아래, 2=왼쪽, 3=오른쪽\n",
    "\n",
    "                if direction == 0 and start_y > 0:  # 위\n",
    "                    start_y -= 1\n",
    "                elif direction == 1 and start_y < self.size - 1:  # 아래\n",
    "                    start_y += 1\n",
    "                elif direction == 2 and start_x > 0:  # 왼쪽\n",
    "                    start_x -= 1\n",
    "                elif direction == 3 and start_x < self.size - 1:  # 오른쪽\n",
    "                    start_x += 1\n",
    "\n",
    "                self.map[start_y, start_x] = 0  # 해당 위치를 육지로 설정\n",
    "                \n",
    "    def reset(self):\n",
    "        self.total_reward = 0  # 환경 초기화 시 누적 보상도 초기화\n",
    "        while True:\n",
    "            # 임의의 시작 위치 설정\n",
    "            \n",
    "            start_x = np.random.randint(0, self.size)\n",
    "            start_y = np.random.randint(0, self.size)\n",
    "            # 위치가 바다이면 선택\n",
    "            if self.map[start_y, start_x] == 1:\n",
    "                self.agent_position = (start_y, start_x)\n",
    "                self.start_position = (start_y, start_x)  # 시작 위치를 저장\n",
    "                break\n",
    "\n",
    "        while True:\n",
    "            # 임의의 끝 위치 설정\n",
    "            end_x = np.random.randint(0, self.size)\n",
    "            end_y = np.random.randint(0, self.size)\n",
    "            # 위치가 바다이고 시작 위치와 다르면 선택\n",
    "            if self.map[end_y, end_x] == 1 and (end_y, end_x) != self.start_position:\n",
    "                self.end_position = (end_y, end_x)  # 끝 위치를 저장\n",
    "                break\n",
    "\n",
    "        return self.get_surrounding_tiles(self.agent_position)\n",
    "    \n",
    "    def get_surrounding_tiles(self, pos):\n",
    "        surrounding_tiles = []\n",
    "\n",
    "        # 주변 타일 정보 추가\n",
    "        for dx in range(-10, 11):  # 에이전트 위치로부터 좌우로 10칸\n",
    "            for dy in range(-10, 11):  # 에이전트 위치로부터 상하로 10칸\n",
    "                if dx == 0 and dy == 0:\n",
    "                    continue\n",
    "                x, y = pos[0] + dx, pos[1] + dy\n",
    "                if 0 <= x < self.size and 0 <= y < self.size:\n",
    "                    surrounding_tiles.append(self.map[x, y])\n",
    "                else:\n",
    "                    surrounding_tiles.append(-1)  # 경계 밖을 나타내는 값\n",
    "\n",
    "        # 시작 위치, 현재 위치, 도착 위치 정보 추가\n",
    "        normalized_start = (self.start_position[0] / self.size, self.start_position[1] / self.size)\n",
    "        normalized_current = (pos[0] / self.size, pos[1] / self.size)\n",
    "        normalized_end = (self.end_position[0] / self.size, self.end_position[1] / self.size)\n",
    "\n",
    "        surrounding_tiles.extend(normalized_start)\n",
    "        surrounding_tiles.extend(normalized_current)\n",
    "        surrounding_tiles.extend(normalized_end)\n",
    "\n",
    "        return np.array(surrounding_tiles)\n",
    "\n",
    "\n",
    "    def step(self, action):\n",
    "        y, x = self.agent_position\n",
    "\n",
    "        if action == 0:  # 위로 이동\n",
    "            y -= 1\n",
    "        elif action == 1:  # 아래로 이동\n",
    "            y += 1\n",
    "        elif action == 2:  # 왼쪽으로 이동\n",
    "            x -= 1\n",
    "        elif action == 3:  # 오른쪽으로 이동\n",
    "            x += 1\n",
    "\n",
    "        done = False  # 종료 플래그 초기화\n",
    "        \n",
    "\n",
    "        \n",
    "        # 맵 바깥으로 나갔는지 확인\n",
    "        if x < 0 or x >= self.size or y < 0 or y >= self.size:\n",
    "            return self.get_surrounding_tiles(self.agent_position), -1000, True, {}\n",
    "\n",
    "        # 육지에 닿았는지 확인\n",
    "        if self.map[y, x] == 0:\n",
    "            return self.get_surrounding_tiles(self.agent_position), -1000, True, {}\n",
    "\n",
    "        # 도착점에 도달했는지 확인\n",
    "        if (y, x) == self.end_position:\n",
    "            return self.get_surrounding_tiles(self.agent_position), 1000, True, {}\n",
    "\n",
    "        # 현재 상태와 다음 상태에서의 도착점까지의 거리 계산\n",
    "        current_distance = np.linalg.norm(np.array(self.agent_position) - np.array(self.end_position))\n",
    "        next_distance = np.linalg.norm(np.array([y, x]) - np.array(self.end_position))\n",
    "        \n",
    "        # 거리가 줄어들면 보상\n",
    "        if next_distance < current_distance:\n",
    "            reward = +0\n",
    "        else:\n",
    "            reward = -0\n",
    "            \n",
    "        # 최근 방문 위치 패널티\n",
    "        if (y, x) in self.recently_visited:\n",
    "            reward -= 30\n",
    "            \n",
    "        self.recently_visited.append((y, x))\n",
    "        if len(self.recently_visited) > 5:  # 최근 5개의 위치만 저장\n",
    "            self.recently_visited.pop(0)\n",
    "            \n",
    "        self.total_reward += reward\n",
    "        \n",
    "        if self.total_reward <= -self.stop_n:\n",
    "            done = True\n",
    "        '''\n",
    "        elif self.total_reward >= 500:\n",
    "            done = True\n",
    "        '''\n",
    "        self.agent_position = (y, x)\n",
    "        return self.get_surrounding_tiles(self.agent_position), reward, done, {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-13T08:09:47.719515Z",
     "iopub.status.busy": "2023-10-13T08:09:47.719069Z",
     "iopub.status.idle": "2023-10-13T08:09:47.726424Z",
     "shell.execute_reply": "2023-10-13T08:09:47.725252Z",
     "shell.execute_reply.started": "2023-10-13T08:09:47.719483Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_frame(env):\n",
    "    fig, ax = plt.subplots()\n",
    "    \n",
    "    map_visual = np.copy(env.map)\n",
    "    map_visual[env.agent_position[0], env.agent_position[1]] = 2\n",
    "    map_visual[env.start_position[0], env.start_position[1]] = 3\n",
    "    map_visual[env.end_position[0], env.end_position[1]] = 4\n",
    "    \n",
    "    colors = ['black', 'gray', 'yellow', 'lime', 'gold']  # 색상 추가: 육지, 바다, 에이전트, 시작점, 도착점\n",
    "    ax.imshow(map_visual, cmap=plt.cm.colors.ListedColormap(colors))\n",
    "    \n",
    "    plt.axis('off')\n",
    "    canvas = FigureCanvas(fig)\n",
    "    canvas.draw()\n",
    "    img_arr = np.array(canvas.renderer.buffer_rgba())\n",
    "    plt.close()\n",
    "    return img_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# DQN 네트워크 정의\n",
    "class DQNetwork(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(DQNetwork, self).__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(input_dim, 1024),\n",
    "            nn.LeakyReLU(),    # LeakyReLU 활성화 함수 추가\n",
    "            nn.Dropout(0.2),       # 드롭아웃 추가\n",
    "\n",
    "            nn.Linear(1024, 1024),\n",
    "            nn.LeakyReLU(),    # LeakyReLU 활성화 함수 추가\n",
    "            nn.Dropout(0.2),       # 드롭아웃 추가\n",
    "            \n",
    "            nn.Linear(1024, 1024),\n",
    "            nn.LeakyReLU(),    # LeakyReLU 활성화 함수 추가\n",
    "            nn.Dropout(0.2),       # 드롭아웃 추가\n",
    "            \n",
    "            nn.Linear(1024, 1024),\n",
    "            nn.LeakyReLU(),    # LeakyReLU 활성화 함수 추가\n",
    "            nn.Dropout(0.2),       # 드롭아웃 추가\n",
    "\n",
    "            nn.Linear(1024, output_dim)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "    def append_memory(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "# 에이전트 클래스 정의\n",
    "class DQNAgent:\n",
    "    def __init__(self, input_dim, output_dim, learning_rate, gamma, epsilon, device, batch_size):\n",
    "        self.dqn = DQNetwork(input_dim, output_dim).to(device)\n",
    "        self.target_dqn = DQNetwork(input_dim, output_dim).to(device)\n",
    "        self.target_dqn.load_state_dict(self.dqn.state_dict())\n",
    "        self.target_dqn.eval()\n",
    "        self.optimizer = optim.Adam(self.dqn.parameters(), lr=learning_rate)\n",
    "        self.loss_fn = nn.MSELoss()\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.device = device\n",
    "        self.batch_size = batch_size\n",
    "        self.memory = deque(maxlen=300000)\n",
    "    \n",
    "    def get_action(self, state, epsilon):\n",
    "        if random.random() < self.epsilon:\n",
    "            return random.randint(0, 3)\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                state = torch.FloatTensor(state).to(self.device)\n",
    "                q_values = self.dqn(state)\n",
    "                return torch.argmax(q_values).item()\n",
    "\n",
    "    def train(self, batch_size):\n",
    "        if len(self.memory) < self.batch_size:\n",
    "            return\n",
    "\n",
    "        batch = random.sample(self.memory, self.batch_size)\n",
    "        states, actions, rewards, next_states, dones = zip(*batch)\n",
    "        \n",
    "        states = torch.FloatTensor(np.array(states).to(self.device)\n",
    "        actions = torch.LongTensor(np.array(actions) ).to(self.device)\n",
    "        rewards = torch.FloatTensor(np.array(rewards) ).to(self.device)\n",
    "        next_states = torch.FloatTensor(np.array(next_states)).to(self.device)\n",
    "        dones = torch.FloatTensor(np.array(dones, dtype=np.float32)).to(self.device)\n",
    "\n",
    "        q_values = self.dqn(states)\n",
    "        next_q_values = self.target_dqn(next_states).detach()\n",
    "        next_q_value = next_q_values.max(1)[0]\n",
    "\n",
    "        expected_q_values = rewards + (1 - dones) * self.gamma * next_q_value\n",
    "        loss = self.loss_fn(q_values.gather(1, actions.unsqueeze(1)), expected_q_values.unsqueeze(1))\n",
    "\n",
    "\n",
    "        with amp.scale_loss(loss, self.optimizer) as scaled_loss:\n",
    "            scaled_loss.backward()\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "\n",
    "    def update_target(self):\n",
    "        self.target_dqn.load_state_dict(self.dqn.state_dict())\n",
    "\n",
    "    def append_memory(self, state, action, reward, next_state, done):\n",
    "        state = np.array(state)\n",
    "        next_state = np.array(next_state)\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 환경 초기화 및 에이전트 생성\n",
    "env = SimpleOceanEnv(30, 400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "view_tpye = False # True 평가모드\n",
    "create_fst = True # 첫 시도 시 True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4096\n",
    "if view_tpye == True:\n",
    "    epsilon = 0\n",
    "    epsilon_decay = 0\n",
    "    min_epsilon = 0\n",
    "else:\n",
    "    epsilon = 0.6\n",
    "    epsilon_decay = 0.995\n",
    "    min_epsilon = 0.1\n",
    "\n",
    "update_target_interval = 5\n",
    "\n",
    "agent = DQNAgent(446, 4, learning_rate=1e-3,gamma=0.99, epsilon=epsilon, device=device, batch_size=batch_size)\n",
    "# 미리 저장된 모델의 가중치 불러오기\n",
    "try:\n",
    "    if create_fst == False:\n",
    "        agent.dqn.load_state_dict(torch.load('S_90p_231104_visited_v5_False.pth'))\n",
    "        if view_tpye == True:\n",
    "            agent.dqn.eval()  # 모델을 평가 모드로 전환\n",
    "            print('mode type : 평가모드')\n",
    "        else:\n",
    "            print('mode type : 학습모드')\n",
    "        print('load_model 성공')\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# 학습 루프\n",
    "if view_tpye == True:\n",
    "    num_episodes = 30\n",
    "else:\n",
    "    num_episodes = 90000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.dqn, agent.optimizer = amp.initialize(agent.dqn, agent.optimizer, opt_level=\"O1\")\n",
    "frames = []\n",
    "total_mean = 0\n",
    "for episode in tqdm(range(num_episodes)):\n",
    "    state = env.reset()\n",
    "    total_reward = 0\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "        \n",
    "        action = agent.get_action([state], epsilon)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        agent.append_memory(state, action, reward, next_state, done)\n",
    "        agent.train(batch_size)\n",
    "        total_reward += reward\n",
    "        state = next_state\n",
    "        if view_tpye == True:\n",
    "            if episode % ((num_episodes+1)%2) == 0:\n",
    "                frame_data = {\n",
    "                    'frame': get_frame(env),\n",
    "                    'episode': episode,\n",
    "                    'reward': total_reward,\n",
    "                    'epsilon': epsilon\n",
    "                }\n",
    "                frames.append(frame_data)\n",
    "        total_mean+=total_reward\n",
    "        \n",
    "        #print(f\"Episode: {episode}, Total Reward: {total_reward}, mean:{round(np.mean(total_mean/episode), 5)}\")\n",
    "        #clear_output(wait=True)\n",
    "    #clear_output(wait=True)\n",
    "    if episode % update_target_interval == 0:\n",
    "        agent.update_target()\n",
    "    epsilon = max(epsilon * epsilon_decay, min_epsilon)\n",
    "    #print(f\"Episode: {episode}, Total Reward: {total_reward}, Epsilon: {epsilon:.2f}\")\n",
    "\n",
    "env.close()\n",
    "torch.save(agent.dqn.state_dict(), f'S_90p_231104_visited_v5_{view_tpye}.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for frame_data in frames:\n",
    "    fig, ax = plt.subplots()\n",
    "    im = ax.imshow(frame_data['frame'])\n",
    "    \n",
    "    ax.text(1, 1, f\"Episode: {frame_data['episode']} | Reward: {frame_data['reward']} | epsilon: {frame_data['epsilon']}\", \n",
    "            color='white', fontsize=12, bbox=dict(facecolor='black', alpha=0.7))\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "    time.sleep(0.01)\n",
    "    clear_output(wait=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "학습을 시킬떄 모델을 생성하고 생성된 모델에서 환경을 조절하여 학습을 이어나가는 게 맞다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "다 쳐봐야 아는 것인디..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
